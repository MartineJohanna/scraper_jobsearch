{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time \n",
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import re, html\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functies voor de scraper\n",
    "\n",
    "#get soup\n",
    "def get_soup(text):\n",
    "    return BeautifulSoup(text, \"lxml\", from_encoding=\"utf-8\")\n",
    "\n",
    "def zoek_divs(soup):\n",
    "    divs = soup.find_all(name=\"a\", attrs={'class':'job-result'})\n",
    "    return divs\n",
    "\n",
    "# zoek link van vacature\n",
    "def zoek_link(div):\n",
    "    return div['href']\n",
    "\n",
    "# zoek locatie van de vacature\n",
    "def zoek_plaats(div):\n",
    "    places = div.find_all('span', 'job-result__place')\n",
    "    for place in places:\n",
    "        place = ''.join(place.get_text().split())[:-6]\n",
    "        place = place.replace('(', '')\n",
    "        return place\n",
    "\n",
    "#zoek naam van bedrijf van de vacature\n",
    "def zoek_bedrijven(div):\n",
    "    bedrijven = div.find_all('span', 'dashed-list__item')\n",
    "    for bedrijf in bedrijven:\n",
    "        alles = []\n",
    "        test = bedrijf.get_text()\n",
    "        test = test.replace('Premium', '')\n",
    "        test = test.replace('Nieuw', '')\n",
    "        test = test.rstrip()\n",
    "        alles.append(test)\n",
    "        alles = ''.join(str(alles).split())[2:-4]\n",
    "    return alles\n",
    "\n",
    "#zoek inhoud gehele vacature\n",
    "def hele_vac(soup):\n",
    "    divs = soup.findAll(\"div\",attrs={\"class\":\"g-row-2\"})\n",
    "    text = []\n",
    "    for div in divs:\n",
    "        text.append(div.text.strip())\n",
    "    return text\n",
    "\n",
    "#zoek titel van de vacature\n",
    "def zoek_titels(div):\n",
    "    titles = div.find(name='h2', attrs={'class':\"job-result__title\"}).get_text()\n",
    "    return titles\n",
    "\n",
    "#zoek paginanummer van de vacature\n",
    "def zoek_pagina(soup):\n",
    "    pagina = soup.find(name='li', attrs={'class':'page-item active'}).get_text()\n",
    "    pagina = int(pagina)\n",
    "    return pagina\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "titel=[]\n",
    "bedrijf=[]\n",
    "plaats=[]\n",
    "link=[]\n",
    "\n",
    "# scrapen van elke pagina\n",
    "\n",
    "for i in range(1,1000):\n",
    "    xrnd = np.random.uniform(3, 6)\n",
    "    time.sleep(xrnd)\n",
    "    \n",
    "    page = requests.get('https://www.jobbird.com/nl/vacature?s=data%20scientist&p=Nederland&rad=alles&page='+ str(i) +'&ot=relevance')\n",
    "    soup = get_soup(page.text)\n",
    "    \n",
    "    if(zoek_pagina(soup) < i):\n",
    "        break\n",
    "    \n",
    "    divs = soup.find_all(name=\"a\", attrs={'class':'job-result'})\n",
    "    \n",
    "    if(len(divs) == 0):\n",
    "        break\n",
    "        \n",
    "        \n",
    "    for div in divs:\n",
    "        titel.append(zoek_titels(div))\n",
    "        bedrijf.append(zoek_bedrijven(div))\n",
    "        plaats.append(zoek_plaats(div))\n",
    "        link.append(zoek_link(div))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe van de resultaten\n",
    "df = pd.DataFrame(\n",
    "    {'titel': titel,\n",
    "     'bedrijf': bedrijf,\n",
    "     'plaats': plaats,\n",
    "     'link': link\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verwijderen van duplicates\n",
    "df = df.drop_duplicates(subset='link', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapen van de inhoud voor elke vacature\n",
    "df2 = pd.DataFrame([])\n",
    "vacs = []\n",
    "cnt = -1\n",
    "for _, vac in df.iterrows():   \n",
    "    xrnd = np.random.uniform(3, 6)\n",
    "    time.sleep(xrnd)\n",
    "    \n",
    "    num = (cnt + 1) \n",
    "    \n",
    "    cnt = cnt + 1\n",
    "#     if cnt > 10:\n",
    "#         break;\n",
    "        \n",
    "    page = requests.get(vac['link'])\n",
    "    soup = get_soup(page.text)\n",
    "    alles = []\n",
    "    \n",
    "    alles.append(hele_vac(soup))\n",
    "    df2[num] = alles\n",
    "    \n",
    "df2 = df2.T\n",
    "df2.rename(columns={df2.columns[0]: \"alles\"}, inplace=True)\n",
    "result = df.merge(df2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['plaats'] = df['plaats'].replace({'[^a-zA-Z]':' '},regex=True)\n",
    "df['titel'] = df['titel'].replace({'[^a-zA-Z]':' '},regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('jobbird_nederland.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
